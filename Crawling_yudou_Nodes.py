import requests
from bs4 import BeautifulSoup
import re
import os
from datetime import datetime

# è·å–æ–‡ç« åˆ—è¡¨é¡µçš„æ‰€æœ‰æ–‡ç« é“¾æ¥
def get_article_links(base_url="https://www.yudou66.cc/"):
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(base_url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")
    links = [a['href'] for a in soup.select("article h2 a") if a['href'].startswith("http")]
    return links

# æå–txté“¾æ¥
def get_txt_link(article_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(article_url, headers=headers)
    if resp.status_code != 200:
        return None
    match = re.search(r'https://yy\.yudou66\.top/\d{6}/\d{8}.*?\.txt', resp.text)
    return match.group(0) if match else None

# éªŒè¯txtæ–‡ä»¶å†…å®¹æ˜¯å¦æœ‰æ•ˆ
def is_valid_txt(content):
    # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºï¼Œæˆ–è€…æ˜¯å¦åŒ…å«æ— æ•ˆçš„è¯è¯­ï¼ˆä¾‹å¦‚â€œæµ‹è¯•â€ï¼Œâ€œåˆ é™¤â€ç­‰ï¼‰
    if not content.strip():  # å†…å®¹ä¸ºç©º
        return False
    invalid_keywords = ["æµ‹è¯•", "åˆ é™¤", "åˆæˆ"]
    for keyword in invalid_keywords:
        if keyword in content:
            return False
    return True

# ä¸‹è½½å¹¶ä¿å­˜æœ‰æ•ˆçš„txtæ–‡ä»¶ï¼Œä½¿ç”¨æ—¶é—´æˆ³ç¡®ä¿æ–‡ä»¶åå”¯ä¸€
def download_and_save_txt(url):
    filename = url.split("/")[-1]
    
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.realpath(__file__))
    
    # ä¸ºäº†ç¡®ä¿æ–‡ä»¶åå”¯ä¸€ï¼Œæ·»åŠ æ—¶é—´æˆ³ï¼ˆå¹´æœˆæ—¥æ—¶åˆ†ç§’ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"{timestamp}_{filename}"
    
    # æ„å»ºä¿å­˜æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    file_path = os.path.join(script_dir, filename)
    
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            content = response.text
            # éªŒè¯æ–‡ä»¶å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            if is_valid_txt(content):
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content)
                print(f"âœ… ä¿å­˜æˆåŠŸ: {file_path}")
            else:
                print("âŒ æ–‡ä»¶å†…å®¹æ— æ•ˆï¼Œæœªä¿å­˜")
        else:
            print(f"âŒ æ— æ³•ä¸‹è½½: {url}")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥: {url}ï¼Œé”™è¯¯ä¿¡æ¯: {e}")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("ğŸ“¥ å¼€å§‹çˆ¬å–æ–‡ç« é¡µé¢...")
    articles = get_article_links()
    print(f"ğŸ”— å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹å¤„ç†ç¬¬1ç¯‡...")

    # åªçˆ¬å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„txtæ–‡ä»¶
    for url in articles:
        print(f"\nğŸ” å¤„ç†æ–‡ç« : {url}")
        txt_link = get_txt_link(url)
        if txt_link:
            print(f"ğŸ“„ æ‰¾åˆ°TXTé“¾æ¥: {txt_link}")
            download_and_save_txt(txt_link)
            break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶åå°±åœæ­¢
        else:
            print("ğŸš« æ²¡æœ‰æ‰¾åˆ°TXTé“¾æ¥")
